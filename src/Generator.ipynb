{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "##Installing requirements\n",
    "!pip install tensorflow==2.0.0beta numpy pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement GAN and VAE for generator class\n",
    "\n",
    "class GAN(tf.keras.Model):\n",
    "    def __init__(self, batch_size):\n",
    "        super(GAN, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.cross_entropy = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "        self.generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "        self.discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "        \n",
    "        self.generator = self.make_generator_GAN()\n",
    "        self.discriminator = self.make_discriminator_GAN()\n",
    "        \n",
    "    def make_generator_GAN(self):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(500, use_bias=False, input_shape=(100,)))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.LeakyReLU())\n",
    "          \n",
    "        model.add(tf.keras.layers.Dense(250, use_bias=False))\n",
    "        model.add(tf.keras.layers.Dense(200, use_bias=False))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.LeakyReLU())\n",
    "    \n",
    "        model.add(tf.keras.layers.Dense(50, use_bias=False))    \n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.LeakyReLU())\n",
    "    \n",
    "        model.add(tf.keras.layers.Dense(131, activation=\"tanh\", use_bias=False))      \n",
    "        return model\n",
    "    \n",
    "    def make_discriminator_GAN(self):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(100, input_shape=(131,)))\n",
    "        model.add(tf.keras.layers.LeakyReLU())\n",
    "        model.add(tf.keras.layers.Dropout(0.3))\n",
    "          \n",
    "        model.add(tf.keras.layers.Dense(50))\n",
    "        model.add(tf.keras.layers.LeakyReLU())\n",
    "        model.add(tf.keras.layers.Dropout(0.3))\n",
    "           \n",
    "        model.add(tf.keras.layers.Dense(1))\n",
    "         \n",
    "        return model\n",
    "    \n",
    "    def generator_loss(self, fake_output):\n",
    "        return self.cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "    \n",
    "    def discriminator_loss(self, real_output, fake_output):\n",
    "        real_loss = self.cross_entropy(tf.ones_like(real_output), real_output)\n",
    "        fake_loss = self.cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "        total_loss = real_loss + fake_loss\n",
    "        return total_loss\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, dataset):\n",
    "        noise = tf.random.normal([self.batch_size, 100])\n",
    "    \n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "          generated_data = self.generator(noise, training=True)\n",
    "    \n",
    "          real_output = self.discriminator(dataset, training=True)\n",
    "          fake_output = self.discriminator(generated_data, training=True)\n",
    "    \n",
    "          gen_loss = self.generator_loss(fake_output)\n",
    "          disc_loss = self.discriminator_loss(real_output, fake_output)\n",
    "    \n",
    "        gradients_of_generator = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
    "        gradients_of_discriminator = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
    "    \n",
    "        self.generator_optimizer.apply_gradients(zip(gradients_of_generator, self.generator.trainable_variables))\n",
    "        self.discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, self.discriminator.trainable_variables))\n",
    "    \n",
    "    def train(self, dataset, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            for data_batch in dataset:\n",
    "                self.train_step(data_batch)\n",
    "                \n",
    "    def generate(self, test_data, epochs):\n",
    "        while epochs > 0:\n",
    "            yield self.generator(test_data, training=False)\n",
    "            epochs -= 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self, latent_dim, batch_size):\n",
    "        super(VAE, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "        self.inference_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "        \n",
    "        self.inference_net = self.inference_ae()\n",
    "        self.generative_net = self.generative_ae()\n",
    "    \n",
    "    def inference_ae(self):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(32, use_bias=False, input_shape=(131,)))\n",
    "        model.add(tf.keras.layers.LeakyReLU())\n",
    "        model.add(tf.keras.layers.Dense(48, use_bias=False))\n",
    "        model.add(tf.keras.layers.LeakyReLU())\n",
    "              #tf.keras.layers.BatchNormalization(),\n",
    "        model.add(tf.keras.layers.Dense(64, use_bias=False))\n",
    "        model.add(tf.keras.layers.LeakyReLU())\n",
    "              #tf.keras.layers.BatchNormalization(),\n",
    "              # No activation\n",
    "        model.add(tf.keras.layers.Dense(self.latent_dim + self.latent_dim, use_bias=False))\n",
    "        return model\n",
    "    \n",
    "    def generative_ae(self):\n",
    "        model = tf.keras.Sequential()\n",
    "\n",
    "        model.add(tf.keras.layers.Dense(64, use_bias=False, input_shape=(None, self.latent_dim)))\n",
    "        model.add(tf.keras.layers.LeakyReLU())\n",
    "        model.add(tf.keras.layers.Dense(48, use_bias=False))\n",
    "              #tf.keras.layers.BatchNormalization(),\n",
    "        model.add(tf.keras.layers.LeakyReLU())\n",
    "        model.add(tf.keras.layers.Dense(32, use_bias=False))\n",
    "        model.add(tf.keras.layers.LeakyReLU())\n",
    "              #tf.keras.layers.BatchNormalization(),\n",
    "              # No activation\n",
    "        model.add(tf.keras.layers.Dense(131, use_bias=False))\n",
    "        return model\n",
    "        \n",
    "    def sample(self, eps=None):\n",
    "        if eps is None:\n",
    "          eps = tf.random.normal(shape=(100, self.latent_dim))\n",
    "        return self.decode(eps, train=False, apply_sigmoid=True)\n",
    "    \n",
    "    def encode(self, x, train=True):\n",
    "        mean, logvar = tf.split(self.inference_net(x, training=train), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "    \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "    \n",
    "    def decode(self, z, train=True, apply_sigmoid=False):\n",
    "        logits = self.generative_net(z, training=train)\n",
    "        if apply_sigmoid:\n",
    "          probs = tf.sigmoid(logits)\n",
    "          return probs\n",
    "    \n",
    "        return logits\n",
    "    \n",
    "    def log_normal_pdf(self, sample, mean, logvar, raxis=1):\n",
    "          log2pi = tf.math.log(2. * np.pi)\n",
    "          return tf.reduce_sum(-.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi), axis=raxis)\n",
    "\n",
    "    \n",
    "    def train_step(self, dataset):\n",
    "    \n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            dataset = tf.cast(dataset, tf.float32)\n",
    "            mean, logvar = self.encode(dataset, True)\n",
    "            z = self.reparameterize(mean, logvar)\n",
    "            x_logit = self.decode(z, True)\n",
    "        \n",
    "            cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=dataset)\n",
    "            logpx_z = -tf.reduce_sum(cross_ent, axis=1)\n",
    "            logpz = self.log_normal_pdf(z, 0., 0.)\n",
    "            logqz_x = self.log_normal_pdf(z, mean, logvar)\n",
    "            loss = -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
    "    \n",
    "        gradients_of_generative = gen_tape.gradient(loss, self.generative_net.trainable_variables)\n",
    "        gradients_of_inference = disc_tape.gradient(loss, self.inference_net.trainable_variables)\n",
    "    \n",
    "        self.generator_optimizer.apply_gradients(zip(gradients_of_generative, self.generative_net.trainable_variables))\n",
    "        self.inference_optimizer.apply_gradients(zip(gradients_of_inference, self.inference_net.trainable_variables))\n",
    "    \n",
    "    def train(self, dataset, epochs):\n",
    "        #self.model = VAE(self.latent_dim, self.batch_size)\n",
    "        \n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            for data_batch in dataset:\n",
    "                self.train_step(data_batch)\n",
    "                \n",
    "    def generate(self, test_data, epochs):\n",
    "        while epochs > 0:\n",
    "            yield self.sample(test_data)\n",
    "            epochs -= 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generator:\n",
    "class Generator(object):\n",
    "    def __init__(self, data, p, batch_size):\n",
    "        self.data = data\n",
    "        self.p = p\n",
    "        self.batch_size = batch_size\n",
    "        self.gan = GAN(self.batch_size)\n",
    "        self.vae = VAE(100, self.batch_size)\n",
    "    \n",
    "    def train(self, latent_dim, epochs):\n",
    "        \n",
    "        self.gan.train(self.data, epochs)\n",
    "        \n",
    "        self.vae.train(self.data, epochs)\n",
    "        \n",
    "    def generate(self, test_data, epochs):\n",
    "        while epochs > 0:\n",
    "            samp = np.random.random_sample()\n",
    "            if self.p >= samp:\n",
    "                yield next(self.gan.generate(test_data, epochs))\n",
    "            else:\n",
    "                yield next(self.vae.generate(test_data, epochs))\n",
    "            epochs -= 1\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading and preprocessing data\n",
    "data = pd.read_csv(\"../data/ds_test.csv\")\n",
    "#MinMax Scaling for avoiding problems with VAE\n",
    "scaler = MinMaxScaler()\n",
    "data.loc[:,\"amount\"] = scaler.fit_transform(np.array(data.amount).reshape(-1,1))\n",
    "#storing date and description for posterior mappings\n",
    "date = data.date\n",
    "\n",
    "desc = np.unique(data.description)\n",
    "\n",
    "#store category as label and one-hot encode it\n",
    "\n",
    "category = pd.get_dummies(data.category, prefix=\"category\")\n",
    "date = pd.get_dummies(data.date, prefix=\"date\")\n",
    "description = pd.get_dummies(data.description, prefix=\"description\")\n",
    "\n",
    "#remove currency and ID and old not onehot encoded columns\n",
    "data = data.drop([\"id\", \"currency\", \"category\",\"description\",\"date\"], axis=1)\n",
    "\n",
    "#add new columns\n",
    "data = pd.concat([data, date, description, category], axis=1)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(data.values).shuffle(36).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0618 02:32:03.599898 14912 deprecation.py:323] From D:\\DataScience\\anaconda3\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:182: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "##The idea was to map back the one hot encode in order to structure the data generate (2 hours limit already)\n",
    "generator = Generator(train_dataset, 0.6, 16)\n",
    "generator.train(100, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(70):\n",
    "    next(generator.generate(tf.random.normal([32, 100]), 50))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
